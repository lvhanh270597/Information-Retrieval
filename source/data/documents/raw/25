Đẹp 4
http://www.giaithuatlaptrinh.com/?p=2023
Nơi tổng hợp và chia sẻ những kiến thức liên quan tới giải thuật nói chung và lý thuyết khoa học máy tính nói riêng.   in  | Chủ đề của bài này là bổ đề Johnson-Lindenstrauss [1], một công cụ được sử dụng rất nhiều để giảm chiều dữ liệu. Về các ứng dụng cụ thể của bổ đề mình khuyến khích các bạn tự Google, do ở đây mình không có nhiều không gian để viết về vấn đề đó.Bài này mang tính chất giới thiệu ý tưởng là chính, sau đó là một chút "hương vị" trong chứng minh bổ đề. Lối trình bày của bài này xuất phát từ [8]. Mình sẽ vạch ra con đường để chứng minh và khuyến khích bạn đọc tự "lấp đầy" những đoạn bỏ trống của bài viết. Lời giải đầy đủ của những đoạn bỏ trống mình sẽ liên kết đến tài liệu tham khảo. Trước hết chúng ta phát biểu bổ đề:   
Ở đây  là  . Từ nay về sau, ta viết tắt ohnson-indenstrauss emma là JLL. Trong một số ứng dụng big data, số chiều của dữ liệu rất lớn, xấp xỉ số điểm dữ liệu. Ví dụ  . Sử dụng JLL, ta có thể chiếu lượng dữ liệu này về không gian cỡ  để đảm bảo khoảng cách giữa hai điểm dữ liệu bất kì thay đổi không quá  lần. Ta có thể lấy  thì số chiều cần thiết là không quá 2400, nhỏ hơn nhiều lần số chiều gốc là .  Con số  là con số lý thyết để đảm bảo sự tồn tại của . Con số thực tế cần thiết thường nhỏ hơn rất nhiều (cỡ vài trăm). Phát biểu của bổ đề trên mới đảm bảo tồn tại của hàm . Chứng minh dưới đây sẽ cho chúng ta một thuật toán (ngẫu nhiên) để tìm ra . Bổ đề JLL thường được ứng dụng để tiền xử lí (preprocess) dữ liệu trước khi đi vào xử lí sâu hơn (ví dụ phân loại hoặc phần cụm dữ liệu). JLL chỉ bảo toàn khoảng cách giữa mọi cặp điểm, do đó, chúng ta cần phải cẩn thận trong việc ứng dụng JLL tiền xử lí. Ví dụ ta muốn phân loại dữ liệu thì tiền xử lí bằng JLL chưa chắc đã là tốt (có thể dùng  thay thế). Nếu muốn phân cụm dữ liệu thì có lẽ JLL áp dụng được. Trong phần Example của note [3], Jelani Nelson chứng minh rằng JLL  dùng để tiền xử lí cho .  Theo JLL, . Quan hệ phụ thuộc giữa  và  đã được Larsen và Nelson [5] chứng minh là tối ưu về mặt lý thuyết.   Có hai phiển bản khác của JLL: phiên bản Fast JLL [6] và phiên bản Sparse JLL [7]. Bạn đọc có thể tham khảo thêm trong note [4] của Jelani Nelson.Trong phần này mình sẽ phác họa (sketch) chứng minh (cũng như đi tìm hàm ) của JLL. Lemma 1 dưới đây cho ta biết, hàm  chỉ đơn giản là một ma trận với các phần tử được chọn ngẫu nhiên từ phân bố chuẩn () . Cuối bài mình sẽ thảo luận 2 cách khác để tìm hàm .    Phương pháp chứng minh là sử dụng union-bound. Bạn có thể xem lại một số khái niệm cơ bản về xác suất .Xác suất để một cặp  bất kì  phương trình , theo Lemma 1 (cho ), là không quá . Do đó, xác suất đề  một cặp  bất  phương trình  là không quá . Do đó, xác xuất để  thỏa mãn phương trình   cặp  là :  Xác suất này lớn hơn  khi  . Một sự kiện có xác suất lớn hơn không khi và chỉ khó nó . Điều đó có nghĩa là tồn tại  thỏa mãn JLL. Từ đây về sau, ta sẽ tập trung chứng minh Lemma 1. Bằng cách chia hay vế của bất đẳng thức:  cho , ta có thể giả sử . Do đó, ta chỉ cần chứng minh:  Đặt:  Ta có:   Do , từ  suy ra  chính là một tổ hợp tuyến tính (linear combination) của các biến ngẫu nhiên Gauss đồng nhất và phân phối độc lập. Ta nhắc lại tính chất sau của phân phối :   
Fact 1 kết hợp với quy nạp và điều kiện , ta suy ra:  Ta biến đổi vế trái của  như sau: 
 Ở đây,  là  với  bậc tự do vì nó là tổng bình phương của  biến ngẫu nhiên độc lập có cùng phân bố .Để chứng minh , ta chỉ cần chứng minh:  Mình sẽ dừng chứng minh ở đây vì chứng minh  thực ra là tính tail-bound của phân bố  chi-squared, một bài toán khá cổ điển trong xác suất thống kê. Bạn đọc có thể tiếp cận  bằng 2 cách: (a) sử dụng tích chất của phân phối chi-squared  và (b) sử dụng  và  giống như trong chứng minh của  và . Cách (a) thì bạn chịu khó đọc trang wikipedia của chi-squared còn cách (b) đã được trình bày trong note [8].Có hai cách khác để sinh ra hàm  như trong Lemma 1. Tuy nhiên, chứng minh bổ đề JLL với cả hai cách trọn trên đều không đơn giản. Các bạn có thể tham khảo note [2] của Jiri Matousek.[1] W. B. Johnson and J. Lindenstrauss. . Conference in modern analysis and probability (New Haven, Conn., 1982), 189–206. Contemp. Math 26 (1984).
[2] J. Matousek. .
[3] J. Nelson.  . August, 2015.
[4] J. Nelson.  . August, 2015.
[5] K. G. Larsen and J. Nelson.   (2016).
[6]  N. Ailon and B. Chazelle. . Proceedings of the 38th Annual ACM Symposium on Theory of Computing. pp. 557–563. 2006.
[7] D. M. Kane and J. Nelson. . Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1195–1203. 2012.
[8]  S. Kakade and G. Shakhnarovich, . 2009. , , , , Trackback link: Powered by  and 